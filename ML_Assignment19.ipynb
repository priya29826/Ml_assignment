{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
        "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.**\n",
        "\n",
        "**a) Using the k-means method, create two clusters for each set of centroid described above.**\n",
        "\n",
        "**b) For each set of centroid values, calculate the SSE.**\n",
        "\n",
        "**Ans:**\n",
        "Let's solve this step by step:\n",
        "\n",
        "**a) Using the k-means method to create two clusters for each set of centroids:**\n",
        "\n",
        "**First Set of Centroids: 15, 32**\n",
        "\n",
        "1. Assign Data Points to Clusters:\n",
        "\n",
        "* Cluster 1: {5, 10, 15, 20, 25, 30} (centroid: 17.5)\n",
        "\n",
        "* Cluster 2: {35} (centroid: 35)\n",
        "\n",
        "**Second Set of Centroids: 12, 30**\n",
        "\n",
        "2. Assign Data Points to Clusters:\n",
        "\n",
        "* Cluster 1: {5, 10, 15, 20, 25} (centroid: 15)\n",
        "\n",
        "* Cluster 2: {30, 35} (centroid: 32.5)\n",
        "\n",
        "**b) Now, let's calculate the SSE (Sum of Squared Errors) for each set of centroids:**\n",
        "\n",
        "**First Set of Centroids: 15, 32**\n",
        "\n",
        "* **For Cluster 1 (centroid: 17.5):**\n",
        "\n",
        " * SSE = (5-17.5)^2 + (10-17.5)^2 + (15-17.5)^2 + (20-17.5)^2 + (25-17.5)^2 +   (30-17.5)^2\n",
        " = 152.5 + 112.5 + 6.25 + 6.25 + 112.5 + 152.5\n",
        "\n",
        "= 542.5\n",
        "\n",
        "* **For Cluster 2 (centroid: 35):**\n",
        "\n",
        "* SSE = (35-35)^2= 0\n",
        "\n",
        "Total SSE for the first set of centroids = 542.5\n",
        "\n",
        "**Second Set of Centroids: 12, 30**\n",
        "\n",
        "* **For Cluster 1 (centroid: 15):**\n",
        "\n",
        "   * SSE = (5-15)^2 + (10-15)^2 + (15-15)^2 + (20-15)^2 + (25-15)^2= 100 + 25 +\n",
        "     0 + 25 + 100\n",
        "     = 250\n",
        "\n",
        "* **For Cluster 2 (centroid: 32.5):**\n",
        "\n",
        "   * SSE = (30-32.5)^2 + (35-32.5)^2\n",
        "     = 6.25 + 6.25\n",
        "     = 12.5\n",
        "\n",
        "Total SSE for the second set of centroids = 250 + 12.5 = 262.5\n",
        "\n",
        "So, the SSE values for the two sets of centroids are:\n",
        "\n",
        "First set of centroids: 542.5\n",
        "\n",
        "Second set of centroids: 262.5\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WR8_bxUmPf4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Describe how the Market Basket Research makes use of association analysis concepts.**\n",
        "\n",
        "**Ans:** Market Basket Analysis (MBA) is a technique used in retail and e-commerce to uncover relationships between products that are frequently purchased together. It makes use of association analysis concepts to identify patterns and associations within transaction data. Here's how Market Basket Research utilizes association analysis concepts:\n",
        "\n",
        "**1. Association Rule Mining:** Market Basket Research employs association rule mining algorithms, such as the Apriori algorithm, to discover interesting relationships between items in transaction data. These algorithms analyze the co-occurrence of items in transactions to identify frequent itemsets and generate association rules.\n",
        "\n",
        "**2. Frequent Itemsets:** Market Basket Analysis identifies frequent itemsets, which are combinations of items that appear together in transactions above a specified support threshold. These frequent itemsets represent commonly occurring patterns of item co-purchases in the dataset.\n",
        "\n",
        "**3. Association Rules:** Based on the frequent itemsets, Market Basket Analysis generates association rules that describe relationships between items. These rules consist of an antecedent (or premise) and a consequent (or conclusion), where the antecedent indicates the items that are typically purchased together, and the consequent represents the item that is likely to be purchased as a result.\n",
        "\n",
        "**4. Metrics:** Market Basket Research utilizes metrics such as support, confidence, and lift to evaluate the strength and significance of association rules. Support measures the frequency of occurrence of an itemset in the dataset, confidence measures the conditional probability of the consequent given the antecedent, and lift measures the strength of association between the antecedent and consequent beyond what would be expected by chance.\n",
        "\n",
        "**5. Insights for Retail Strategy:** By analyzing association rules generated from Market Basket Analysis, retailers gain insights into customer purchasing behavior and preferences. They can identify cross-selling opportunities, optimize product placement and assortment, design targeted marketing campaigns, and personalize recommendations to enhance customer satisfaction and increase sales revenue.\n",
        "\n",
        "Overall, Market Basket Research leverages association analysis concepts to extract actionable insights from transaction data, enabling retailers to make informed decisions and improve business performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wRvxta1AOyWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Give an example of the Apriori algorithm for learning association rules.**\n",
        "\n",
        "**Ans:** we have a dataset representing transactions at a grocery store, where each transaction consists of items purchased by a customer. We want to use the Apriori algorithm to find association rules indicating which items tend to be purchased together.\n",
        "\n",
        "Here's a simplified example of a transaction dataset:\n",
        "\n",
        "Transaction 1: {milk, bread, eggs}\n",
        "\n",
        "Transaction 2: {bread, butter}\n",
        "\n",
        "Transaction 3: {milk, bread, butter, yogurt}\n",
        "\n",
        "Transaction 4: {eggs, yogurt}\n",
        "\n",
        "Transaction 5: {bread, eggs}\n",
        "\n",
        "Now, let's use the Apriori algorithm to find association rules. We'll set a minimum support threshold of 40% and a minimum confidence threshold of 60%.\n",
        "\n",
        "1. Step 1: Find frequent itemsets (items with support above the minimum threshold):\n",
        "\n",
        "Generate frequent itemsets of size 1:\n",
        "\n",
        "* {milk}: support = 2/5 = 40%\n",
        "\n",
        "* {bread}: support = 4/5 = 80%\n",
        "\n",
        "* {eggs}: support = 3/5 = 60%\n",
        "\n",
        "* {butter}: support = 2/5 = 40%\n",
        "\n",
        "* {yogurt}: support = 2/5 = 40%\n",
        "\n",
        "Prune infrequent itemsets (remove items with support below the threshold).\n",
        "\n",
        "2. Step 2: Generate candidate itemsets of size 2 (using frequent itemsets from the previous step):\n",
        "\n",
        "* {milk, bread}: support = 2/5 = 40%\n",
        "\n",
        "* {milk, eggs}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {milk, butter}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {milk, yogurt}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {bread, eggs}: support = 3/5 = 60%\n",
        "\n",
        "* {bread, butter}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {bread, yogurt}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {eggs, butter}: support = 0/5 = 0% (below threshold, pruned)\n",
        "\n",
        "* {eggs, yogurt}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {butter, yogurt}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "3. Step 3: Generate candidate itemsets of size 3 (using frequent itemsets from the previous step):\n",
        "\n",
        "* {milk, bread, eggs}: support = 1/5 = 20% (below threshold, pruned)\n",
        "\n",
        "* {bread, eggs, butter}: support = 0/5 = 0% (below threshold, pruned)\n",
        "\n",
        "4. Step 4: Generate association rules from frequent itemsets:\n",
        "For each frequent itemset, generate all possible non-empty subsets and calculate the confidence.\n",
        "Prune rules with confidence below the minimum confidence threshold.\n",
        "Here are some of the resulting association rules:\n",
        "\n",
        "* {bread} => {eggs} (confidence = 3/4 = 75%)\n",
        "\n",
        "* {eggs} => {bread} (confidence = 3/3 = 100%)\n",
        "\n",
        "* {bread} => {milk} (confidence = 2/4 = 50%)\n",
        "\n",
        "* {milk} => {bread} (confidence = 2/2 = 100%)\n",
        "\n",
        "These association rules indicate the likelihood of one item being purchased given the purchase of another item, based on the transaction data and the specified thresholds.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ve96JXRETka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
        "is used to decide when to end the iteration.**\n",
        "\n",
        "**Ans:** In hierarchical clustering, the distance between clusters is measured using various metrics, depending on the linkage method chosen. The most common linkage methods include:\n",
        "\n",
        "**1. Single Linkage (Minimum Linkage):** The distance between two clusters is defined as the shortest distance between any two points in the two clusters.\n",
        "\n",
        "**2. Complete Linkage (Maximum Linkage):** The distance between two clusters is defined as the maximum distance between any two points in the two clusters.\n",
        "\n",
        "**3. Average Linkage:** The distance between two clusters is defined as the average distance between all pairs of points in the two clusters.\n",
        "\n",
        "**4. Centroid Linkage:** The distance between two clusters is defined as the distance between their centroids (the mean of all points in the clusters).\n",
        "\n",
        "**5. Ward's Linkage:** This method minimizes the variance when merging clusters and is based on the increase in within-cluster variance when merging two clusters.\n",
        "\n",
        "Once the distance between clusters is determined, hierarchical clustering iteratively merges clusters based on the smallest (or largest, depending on the linkage method) distance between them. This process continues until a stopping criterion is met, which typically involves reaching a desired number of clusters or until a specific distance threshold is reached.\n",
        "\n",
        "The decision to end the iteration is based on a threshold value of the inter-cluster distance or dissimilarity measure. This threshold is often set by the user and represents the maximum allowable distance at which clusters can be merged. Once the distance between the closest clusters exceeds this threshold, the algorithm stops, and the resulting clusters are considered the final output.\n",
        "\n",
        "Alternatively, hierarchical clustering can also be represented using a dendrogram, which visually displays the sequence of cluster merges and the corresponding distances. The height of the dendrogram at which clusters are merged provides a visual representation of the distance threshold and can aid in determining the appropriate number of clusters or when to end the iteration.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zK7-ip1eEIwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. In the k-means algorithm, how do you recompute the cluster centroids?**\n",
        "\n",
        "**Ans:** In the k-means algorithm, after initially assigning data points to clusters based on their proximity to centroids, the cluster centroids are recomputed iteratively. Here's how the cluster centroids are recomputed:\n",
        "\n",
        "**1. Initialization:** Start by initializing the cluster centroids. This can be done randomly by selecting K data points from the dataset as the initial centroids, or through other methods such as k-means++ initialization.\n",
        "\n",
        "**2. Assignment Step:** Assign each data point to the nearest cluster centroid based on a distance metric, commonly Euclidean distance. This step forms K clusters.\n",
        "\n",
        "**3. Update Step:** After all data points have been assigned to clusters, compute the mean of the data points within each cluster. This mean becomes the new centroid for that cluster.\n",
        "\n",
        "**4. Repeat:** Repeat the assignment and update steps iteratively until convergence criteria are met. Convergence criteria are typically based on changes in centroids or the assignment of data points to clusters.\n",
        "Mathematically, the update step for recomputing the centroid\n",
        "ci for cluster i can be represented as:\n",
        "\n",
        "ùëêùëñ=(1/‚à£ùëÜùëñ‚à£)‚àëùë•‚ààùëÜùëñùë•\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "* ci is the centroid of cluster i\n",
        "* ‚à£Si‚à£ is the number of data points assigned to cluster i\n",
        "* ‚àëx‚ààSi x is the sum of all data points assigned to cluster i\n",
        "\n",
        "This process ensures that the cluster centroids move towards the center of their respective clusters in each iteration, effectively minimizing the within-cluster sum of squares (WCSS) and improving the clustering quality.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LX7UqEY1DUOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
        "clusters.**\n",
        "\n",
        "**Ans:** One method for determining the required number of clusters at the start of a clustering exercise is the \"Elbow Method.\" The Elbow Method is a graphical technique that helps identify the optimal number of clusters by examining the relationship between the number of clusters and the within-cluster sum of squares (WCSS), also known as inertia or distortion.\n",
        "\n",
        "Here's how the Elbow Method works:\n",
        "\n",
        "**1. Run K-Means with Various Values of K:** Start by running the K-means algorithm with different values of K (the number of clusters), ranging from 1 to a certain maximum value.\n",
        "\n",
        "**2. Compute WCSS for Each K:** For each value of K, compute the within-cluster sum of squares (WCSS), which represents the sum of squared distances between each data point and its assigned centroid within the cluster.\n",
        "\n",
        "**3. Plot the Elbow Curve:** Plot a line graph with K on the x-axis and the corresponding WCSS values on the y-axis. As the number of clusters increases, WCSS typically decreases because more clusters will naturally reduce the distances between points and their centroids. However, beyond a certain point, the rate of decrease in WCSS slows down.\n",
        "\n",
        "**4. Identify the \"Elbow\" Point:** Look for the \"elbow\" point on the curve, where the rate of decrease in WCSS sharply levels off. The elbow point represents the optimal number of clusters, where adding more clusters doesn't significantly decrease WCSS.\n",
        "\n",
        "**5. Select the Optimal Number of Clusters:** Choose the value of K at the elbow point as the optimal number of clusters for your dataset.\n",
        "\n",
        "By using the Elbow Method, you can make an informed decision about the appropriate number of clusters for your clustering analysis. However, it's essential to remember that the Elbow Method provides a heuristic and subjective approach, and the optimal number of clusters may vary depending on the specific characteristics of your data and the problem domain. Therefore, it's advisable to combine the Elbow Method with domain knowledge and other validation techniques for robust clustering results."
      ],
      "metadata": {
        "id": "ndzrjHgyCROm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Discuss the k-means algorithm&#39;s advantages and disadvantages.**\n",
        "\n",
        "**Ans:** The advantages and disadvantages of the k-means algorithm:\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "**1. Simplicity:** K-means is easy to implement and understand, making it accessible to users with varying levels of expertise.\n",
        "\n",
        "**2. Efficiency:** It is computationally efficient and can handle large datasets with a relatively low time complexity, especially compared to hierarchical clustering algorithms.\n",
        "\n",
        "**3. Scalability:** K-means can scale well to a large number of data points, making it suitable for applications with high-dimensional data.\n",
        "\n",
        "**4. Versatility:** It can work with various types of data and distance metrics, allowing users to tailor the algorithm to their specific needs.\n",
        "\n",
        "**5. Convergence:** With a finite number of data points and clusters, the algorithm is guaranteed to converge to a local optimum, ensuring stability in its results.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "**1. Sensitivity to Initial Centroids:** K-means performance can be sensitive to the initial selection of centroids, which may lead to suboptimal clustering results or convergence to local minima.\n",
        "\n",
        "**2. Number of Clusters:** The user needs to specify the number of clusters (k) beforehand, which may not always be known or easy to determine, and the choice of k can significantly affect the clustering outcome.\n",
        "\n",
        "**3. Non-Convex Clusters:** K-means assumes that clusters are convex and isotropic, which means it may struggle with clusters of non-convex shapes or varying densities.\n",
        "\n",
        "**4. Impact of Outliers:** Outliers or noise in the data can significantly affect the clustering results, as k-means assigns each point to the nearest centroid, potentially leading to misallocation of outliers.\n",
        "\n",
        "**5. Assumption of Equal Variance:** K-means assumes that clusters have equal variance, which may not hold true for all datasets, leading to biased clustering results in some cases.\n",
        "\n",
        "Overall, while k-means offers simplicity, efficiency, and scalability, it also has limitations related to sensitivity to initialization, determination of the number of clusters, and assumptions about cluster shapes and variances. It's essential to consider these factors when applying the algorithm to real-world data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lElJH1VsBdzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Draw a diagram to demonstrate the principle of clustering.**\n",
        "\n",
        "**Ans:** let's create a simple diagram to demonstrate the principle of clustering. We'll use a two-dimensional space with some data points and visualize how clustering works.\n",
        "\n",
        "Here's a basic diagram:\n",
        "\n",
        "In this diagram:\n",
        "\n",
        "* We have a two-dimensional space represented by the x and y axes.\n",
        "* There are several data points scattered across this space.\n",
        "* The data points are divided into three clusters: Cluster 1, Cluster 2, and Cluster 3.\n",
        "* Each cluster contains data points that are closer to each other in terms of similarity or some other metric.\n",
        "* The clusters are represented by different colors or markers (e.g., circles, squares).\n",
        "\n",
        "This diagram visually represents the principle of clustering, where data points are grouped together based on their similarities or proximity to each other in the feature space\n",
        "\n"
      ],
      "metadata": {
        "id": "i_W04lWUA9rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |       Cluster 2   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |      ‚óè  ‚óè  ‚óè      |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "--------|-------------------|----------\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |      Cluster 1    |\n",
        "        |                   |\n",
        "        |   ‚óè   ‚óè   ‚óè   ‚óè   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "--------|-------------------|----------\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |                   |\n",
        "        |       Cluster 3   |\n",
        "        |                   |\n",
        "        |   ‚óè  ‚óè  ‚óè  ‚óè  ‚óè   |\n",
        "        |                   |\n"
      ],
      "metadata": {
        "id": "19KrDeaEBHF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
        "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
        "C2, and C3 have the following findings after the first iteration:**\n",
        "\n",
        "C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
        "\n",
        "C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
        "\n",
        "C3: (5,5) and (9,9)\n",
        "\n",
        "**What would the cluster centroids**\n",
        "\n",
        "**Ans:** To find the cluster centroids, we calculate the mean of the data points in each cluster.\n",
        "\n",
        "**For cluster C1:**\n",
        "\n",
        "C1={(2,2),(4,4),(6,6)}\n",
        "\n",
        "**Centroid of C1:**\n",
        "\n",
        "(2+4+6/3, 2+4+6/3)=(4,4)\n",
        "\n",
        "**For cluster C2:**\n",
        "\n",
        "\n",
        "C2={(0,4),(4,0),(0,4),(0,4),(0,4),(0,4),(0,4),(0,4),(0}\n",
        "\n",
        "**Centroid of C2:**\n",
        "\n",
        "(0+4+0+0+0+0+0+0+0/9, 4+0+4+4+4+4+4+4+0/9 )=(0.44,3.11)\n",
        "\n",
        "**For cluster C3:**\n",
        "\n",
        "\n",
        "C3={(5,5),(9,9)}\n",
        "\n",
        "**Centroid of C3:**\n",
        "\n",
        "( 5+9/2, 5+9/2 )=(7,7)\n",
        "\n",
        "So, the cluster centroids after the first iteration would be:\n",
        "\n",
        "Centroid of C1: (4,4)\n",
        "\n",
        "Centroid of C2: (0.44,3.11)\n",
        "\n",
        "Centroid of C3: (7,7)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K6vtzd6A9GeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. In a software project, the team is attempting to determine if software flaws discovered during\n",
        "testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters\n",
        "of related defects. Any new defect formed after the 5 clusters of defects have been identified must\n",
        "be listed as one of the forms identified by clustering. A simple diagram can be used to explain this\n",
        "process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the\n",
        "k-means algorithm.**\n",
        "\n",
        "**Ans:** let's illustrate this process with a simple diagram:\n",
        "\n",
        "**1. Initial Defect Data Points:** You start with 20 defect data points.\n",
        "\n",
        "**2. K-Means Clustering:** Using the k-means algorithm, you cluster these 20 defect data points into 5 clusters based on their similarities in defect details.\n",
        "\n",
        "**3. Identified Clusters:** After clustering, you have 5 distinct clusters, each representing a group of related defects.\n",
        "\n",
        "**4. New Defect Detection:** Any new defect data point that arises after the initial clustering must be compared with the existing clusters.\n",
        "\n",
        "**5. Assignment to Clusters:** The new defect is assigned to one of the existing clusters based on its similarity to the defects within each cluster.\n",
        "Here's a simple diagram to illustrate this process:\n",
        "\n",
        "This diagram outlines the process of identifying and clustering defects in a software project, ensuring that any new defect detected is properly assigned to one of the existing clusters based on its similarity to the clustered defects.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zdZVPioR8edF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    +-------------------------+\n",
        "    |        Defect Data      |\n",
        "    |     (20 data points)    |\n",
        "    +-------------------------+\n",
        "                |\n",
        "                v\n",
        "    +-------------------------+\n",
        "    |     K-Means Clustering   |\n",
        "    |   (5 clusters identified)|\n",
        "    +-------------------------+\n",
        "                |\n",
        "                v\n",
        "    +-------------------------+\n",
        "    |    Identified Clusters   |\n",
        "    |     (Cluster 1 - 5)      |\n",
        "    +-------------------------+\n",
        "                |\n",
        "                v\n",
        "    +-------------------------+\n",
        "    |  New Defect Detection   |\n",
        "    |  (Detect and compare    |\n",
        "    |   new defects)           |\n",
        "    +-------------------------+\n",
        "                |\n",
        "                v\n",
        "    +-------------------------+\n",
        "    |  Assignment to Clusters  |\n",
        "    |  (Assign new defect to   |\n",
        "    |   existing clusters)     |\n",
        "    +-------------------------+\n"
      ],
      "metadata": {
        "id": "nTmwPpwG8_lo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}